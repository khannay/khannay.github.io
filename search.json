[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae: Kevin Hannay",
    "section": "",
    "text": "Machine Learning\nData Science\nCircadian rhythms and sleep\nBlending physiological mathematical models with deep learning"
  },
  {
    "objectID": "cv.html#published",
    "href": "cv.html#published",
    "title": "Curriculum Vitae: Kevin Hannay",
    "section": "Published",
    "text": "Published\n\n2023\nJennette P Moreno, Kevin Hannay, Amy R Goetz, Olivia Walch, and Philip Cheng. (2023) \"Validation of the Entrainment Signal Regularity Index and associations with children's changes in BMI.\" Obesity\n        \n        Published\n    \n\n\n2022\nJennette P Moreno, Kevin Hannay, Olivia Walch, Hafza Dadabhoy, Jessica Christian, Maurice Puyau, Abeer El-Mubasher, Fida Bacha, Sarah R Grant,, Rebekah Julie Park, and Philip Cheng. (2022) \"Estimating circadian phase in elementary school children: leveraging advances in physiologically informed models of circadian entrainment and wearable devices.\" Sleep\n        \n        Published\n    \n\n\n2020\nKevin Hannay and J.P. Moreno. (2020) \"Integrating wearable data into circadian models.\" Current Opinion in Systems Biology\n        \n        Published\n    \nKevin Hannay, D. Forger, and V. Booth. (2020) \"Seasonality and light phase-resetting in the mammalian circadian rhythm.\" Scientific Reports\n        \n        Published\n    \n\n\n2019\nJennette P Moreno, Stephanie J Crowley, Candice A Alfano, Kevin Hannay, Debbe Thompson, and Tom Baranowski. (2019) \"Potential circadian and circannual rhythm contributions to the obesity epidemic in elementary school age children.\" International Journal of Behavioral Nutrition and Physical Activity\n        \n        Published\n    \nKevin Hannay, V. Booth, and D. Forger. (2019) \"Macroscopic models for human circadian rhythms.\" Journal of Biological Rhythms\n        \n        Published\n    \n\n\n2018\nKevin Hannay, D. Forger, and V. Booth. (2018) \"Macroscopic models for networks of coupled biological oscillators.\" Science Advances\n        \n        Published\n    \n\n\n2015\nKevin Hannay, V. Booth, and D. Forger. (2015) \"Collective phase response curves for heterogeneous coupled oscillators.\" Physical Review E\n        \n        Published\n    \n\n\n2008\nKevin Hannay, E. Marcotte, and C. Vogel. (2008) \"Buffering by gene duplicates: an analysis of molecular correlates and evolutionary conservation.\" BMC Genomics\n        \n        Published"
  },
  {
    "objectID": "cv.html#working-papers",
    "href": "cv.html#working-papers",
    "title": "Curriculum Vitae: Kevin Hannay",
    "section": "Working Papers",
    "text": "Working Papers\n\n2023\nCaleb Mayer, Olivia Walch, Danny Forger, and Kevin Hannay. (2023) \"Impact of light schedules and model parameters on the circadian outcomes of individuals.\" Under Review\nKevin Hannay. (2023) \"Deep learning from phase response curve data.\" in prep"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Published\n2023\nJennette P Moreno, Kevin Hannay, Amy R Goetz, Olivia Walch, and Philip Cheng. (2023) \"Validation of the Entrainment Signal Regularity Index and associations with children's changes in BMI.\" Obesity\n        \n        Published\n    \n2022\nJennette P Moreno, Kevin Hannay, Olivia Walch, Hafza Dadabhoy, Jessica Christian, Maurice Puyau, Abeer El-Mubasher, Fida Bacha, Sarah R Grant,, Rebekah Julie Park, and Philip Cheng. (2022) \"Estimating circadian phase in elementary school children: leveraging advances in physiologically informed models of circadian entrainment and wearable devices.\" Sleep\n        \n        Published\n    \n2020\nKevin Hannay and J.P. Moreno. (2020) \"Integrating wearable data into circadian models.\" Current Opinion in Systems Biology\n        \n        Published\n    \nKevin Hannay, D. Forger, and V. Booth. (2020) \"Seasonality and light phase-resetting in the mammalian circadian rhythm.\" Scientific Reports\n        \n        Published\n    \n2019\nJennette P Moreno, Stephanie J Crowley, Candice A Alfano, Kevin Hannay, Debbe Thompson, and Tom Baranowski. (2019) \"Potential circadian and circannual rhythm contributions to the obesity epidemic in elementary school age children.\" International Journal of Behavioral Nutrition and Physical Activity\n        \n        Published\n    \nKevin Hannay, V. Booth, and D. Forger. (2019) \"Macroscopic models for human circadian rhythms.\" Journal of Biological Rhythms\n        \n        Published\n    \n2018\nKevin Hannay, D. Forger, and V. Booth. (2018) \"Macroscopic models for networks of coupled biological oscillators.\" Science Advances\n        \n        Published\n    \n2015\nKevin Hannay, V. Booth, and D. Forger. (2015) \"Collective phase response curves for heterogeneous coupled oscillators.\" Physical Review E\n        \n        Published\n    \n2008\nKevin Hannay, E. Marcotte, and C. Vogel. (2008) \"Buffering by gene duplicates: an analysis of molecular correlates and evolutionary conservation.\" BMC Genomics\n        \n        Published\n    \nWorking Papers\n2023\nCaleb Mayer, Olivia Walch, Danny Forger, and Kevin Hannay. (2023) \"Impact of light schedules and model parameters on the circadian outcomes of individuals.\" Under Review\nKevin Hannay. (2023) \"Deep learning from phase response curve data.\" in prep"
  },
  {
    "objectID": "posts/esri/index.html",
    "href": "posts/esri/index.html",
    "title": "Entrainment Signal Regularity Index: Measuring Circadian Disruption",
    "section": "",
    "text": "Introduction\nBlog post coming soon!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{hannay2023,\n  author = {Hannay, Kevin},\n  title = {Entrainment {Signal} {Regularity} {Index:} {Measuring}\n    {Circadian} {Disruption}},\n  date = {2023-02-02},\n  url = {https://khannay.com/posts/esri},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHannay, Kevin. 2023. “Entrainment Signal Regularity Index:\nMeasuring Circadian Disruption.” February 2, 2023. https://khannay.com/posts/esri."
  },
  {
    "objectID": "posts/pytorch-ode/ode.html",
    "href": "posts/pytorch-ode/ode.html",
    "title": "Differential Equations as a Pytorch Neural Network Layer",
    "section": "",
    "text": "Differential equations are the mathematical foundation for most of modern science. They describe the state of a system using an equation for the rate of change (differential). It is remarkable how many systems can be well described by equations of this form. For example, the physical laws describing motion, electromagnetism and quantum mechanics all take this form. More broadly, differential equations describe chemical reaction rates through the law of mass action, neuronal firing and disease spread through the SIR model.\nThe deep learning revolution has brought with it a new set of tools for performing large scale optimizations over enormous datasets. In this post, we will see how you can use these tools to fit the parameters of a custom differential equation layer in pytorch."
  },
  {
    "objectID": "posts/pytorch-ode/ode.html#what-is-the-problem-we-are-trying-to-solve",
    "href": "posts/pytorch-ode/ode.html#what-is-the-problem-we-are-trying-to-solve",
    "title": "Differential Equations as a Pytorch Neural Network Layer",
    "section": "What is the problem we are trying to solve?",
    "text": "What is the problem we are trying to solve?\nLet’s say we have some time series data y(t) that we want to model with a differential equation. The data takes the form of a set of observations yᵢ at times tᵢ. Based on some domain knowledge of the underlying system we can write down a differential equation to approximate the system.\nIn the most general form this takes the form:\n\\begin{align}\n\\frac{dy}{dt} = f(y,t;\\theta)  \\\\\ny(t_0) = y_0\n\\end{align}\nwhere y is the state of the system, t is time, and \\theta are the parameters of the model. In this post we will assume that the parameters \\theta are unknown and we want to learn them from the data.\nLet’s import the libraries we will need for this post. The only non standard machine learning library we will use the  torchdiffeq  library to solve the differential equations. This library implements numerical differential equation solvers in pytorch.\n\n\nCode\nimport torch \nimport torch.nn as nn\nfrom torchdiffeq import odeint as odeint\nimport pylab as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import Callable, List, Tuple, Union, Optional\nfrom pathlib import Path  \n\n\n\n\nCode\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')"
  },
  {
    "objectID": "posts/pytorch-ode/ode.html#models",
    "href": "posts/pytorch-ode/ode.html#models",
    "title": "Differential Equations as a Pytorch Neural Network Layer",
    "section": "Models",
    "text": "Models\nThe first step of our modeling process is to define the model. For differential equations this means we must choose a form for the function f(y,t;\\theta) and a way to represent the parameters \\theta. We also need to do this in a way that is compatible with pytorch.\nThis means we need to encode our function as a torch.nn.Module class. As you will see this is pretty easy and only requires defining two methods. Lets get started with the first of out three example models.\n\nvan Der Pol Oscillator (VDP)\nWe can define a differential equation system using the  torch.nn.Module  class where the parameters are created using the  torch.nn.Parameter  declaration. This lets pytorch know that we want to accumulate gradients for those parameters. We can also include fixed parameters (don’t want to fit these) by just not wrapping them with this declaration.\nThe first example we will use is the classic VDP oscillator which is a nonlinear oscillator with a single parameter \\mu. The differential equations for this system are:\n\\begin{align}\n\\frac{dX}{dt} &= \\mu(x-\\frac{1}{3}x^3-y)  \\\\\n\\frac{dY}{dt} &= \\frac{x}{\\mu}  \\\\\n\\end{align}\nwhere X and Y are the state variables. The VDP model is used to model everything from electronic circuits to cardiac arrhythmias and circadian rhythms. We can define this system in pytorch as follows:\n\n\nCode\nclass VDP(nn.Module):\n    \"\"\" \n    Define the Van der Pol oscillator as a PyTorch module.\n    \"\"\"\n    def __init__(self, \n                 mu: float, # Stiffness parameter of the VDP oscillator\n                 ):\n        super().__init__() \n        self.mu = torch.nn.Parameter(torch.tensor(mu)) # make mu a learnable parameter\n        \n    def forward(self, \n                t: float, # time index\n                state: torch.TensorType, # state of the system first dimension is the batch size\n                ) -&gt; torch.Tensor: # return the derivative of the state\n        \"\"\" \n            Define the right hand side of the VDP oscillator.\n        \"\"\"\n        x = state[..., 0] # first dimension is the batch size\n        y = state[..., 1]\n        dX = self.mu*(x-1/3*x**3 - y)\n        dY = 1/self.mu*x\n        # trick to make sure our return value has the same shape as the input\n        dfunc = torch.zeros_like(state) \n        dfunc[..., 0] = dX\n        dfunc[..., 1] = dY\n        return dfunc\n    \n    def __repr__(self):\n        \"\"\"Print the parameters of the model.\"\"\"\n        return f\" mu: {self.mu.item()}\"\n\n\nYou only need to define the dunder init method (init) and the forward method. I added a string method repr to pretty print the parameter. The key point here is how we can translate from the differential equation to torch code in the forward method. This method needs to define the right-hand side of the differential equation.\nLet’s see how we can integrate this model using the odeint method from torchdiffeq:\n\n\nCode\nvdp_model = VDP(mu=0.5)\n\n# Create a time vector, this is the time axis of the ODE\nts = torch.linspace(0,30.0,1000)\n# Create a batch of initial conditions \nbatch_size = 30\n# Creates some random initial conditions\ninitial_conditions = torch.tensor([0.01, 0.01]) + 0.2*torch.randn((batch_size,2))\n\n# Solve the ODE, odeint comes from torchdiffeq\nsol = odeint(vdp_model, initial_conditions, ts, method='dopri5').detach().numpy()\n\n\n\n\nCode\nplt.plot(ts, sol[:,:,0], lw=0.5);\nplt.title(\"Time series of the VDP oscillator\");\nplt.xlabel(\"time\");\nplt.ylabel(\"x\");\n\n\n\n\n\nHere is a phase plane plot of the solution (a phase plane plot of a parametric plot of the dynamical state).\n\n\nCode\n# Check the solution\nplt.plot(sol[:,:,0], sol[:,:,1], lw=0.5);\nplt.title(\"Phase plot of the VDP oscillator\");\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\n\n\n\n\n\nThe colors indicate the 30 seperate trajectories in our batch. The solution comes back as a torch tensor with dimensions (time_points, batch number, dynamical_dimension).\n\n\nCode\nsol.shape\n\n\n(1000, 30, 2)\n\n\n\n\nLotka Volterra Predator Prey equations\nAs another example we create a module for the Lotka Volterra predator-prey equations. In the Lotka-Volterra (LV) predator-prey model, there are two primary variables: the population of prey (x) and the population of predators (y). The model is defined by the following equations:\n\\begin{align}\n\\frac{dx}{dt} &= \\alpha x - \\beta xy \\\\\n\\frac{dy}{dt} &= -\\delta y + \\gamma xy \\\\\n\\end{align}\nThe population of prey (x) represents the number of individuals of the prey species present in the ecosystem at any given time. The population of predators (y) represents the number of individuals of the predator species present in the ecosystem at any given time.\nIn addition to the primary variables, there are also four parameters that are used to describe various ecological factors in the model:\n\\alpha represents the intrinsic growth rate of the prey population in the absence of predators. \\beta represents the predation rate of the predators on the prey. \\gamma represents the death rate of the predator population in the absence of prey. \\delta represents the efficiency with which the predators convert the consumed prey into new predator biomass.\nTogether, these variables and parameters describe the dynamics of predator-prey interactions in an ecosystem and are used to mathematically model the changes in the populations of prey and predators over time.\n\n\nCode\nclass LotkaVolterra(nn.Module):\n    \"\"\" \n     The Lotka-Volterra equations are a pair of first-order, non-linear, differential equations\n     describing the dynamics of two species interacting in a predator-prey relationship.\n    \"\"\"\n    def __init__(self,\n                 alpha: float = 1.5, # The alpha parameter of the Lotka-Volterra system\n                 beta: float = 1.0, # The beta parameter of the Lotka-Volterra system\n                 delta: float = 3.0, # The delta parameter of the Lotka-Volterra system\n                 gamma: float = 1.0 # The gamma parameter of the Lotka-Volterra system\n                 ) -&gt; None:\n        super().__init__()\n        self.model_params = torch.nn.Parameter(torch.tensor([alpha, beta, delta, gamma]))\n        \n        \n    def forward(self, t, state):\n        x = state[...,0]      #variables are part of vector array u \n        y = state[...,1]\n        sol = torch.zeros_like(state)\n        \n        #coefficients are part of tensor model_params\n        alpha, beta, delta, gamma = self.model_params    \n        sol[...,0] = alpha*x - beta*x*y\n        sol[...,1] = -delta*y + gamma*x*y\n        return sol\n    \n    def __repr__(self):\n        return f\" alpha: {self.model_params[0].item()}, \\\n            beta: {self.model_params[1].item()}, \\\n                delta: {self.model_params[2].item()}, \\\n                    gamma: {self.model_params[3].item()}\"\n\n\nThis follows the same pattern as the first example, the main difference is that we now have four parameters and store them as a model_params tensor. Here is the integration and plotting code for the predator-prey equations.\n\n\nCode\nlv_model = LotkaVolterra() #use default parameters\nts = torch.linspace(0,30.0,1000) \nbatch_size = 30\n# Create a batch of initial conditions (batch_dim, state_dim) as small perturbations around one value\ninitial_conditions = torch.tensor([[3,3]]) + 0.50*torch.randn((batch_size,2))\nsol = odeint(lv_model, initial_conditions, ts, method='dopri5').detach().numpy()\n# Check the solution\n\nplt.plot(ts, sol[:,:,0], lw=0.5);\nplt.title(\"Time series of the Lotka-Volterra system\");\nplt.xlabel(\"time\");\nplt.ylabel(\"x\");\n\n\n\n\n\nNow a phase plane plot of the system:\n\n\nCode\nplt.plot(sol[:,:,0], sol[:,:,1], lw=0.5);\nplt.title(\"Phase plot of the Lotka-Volterra system\");\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\n\n\n\n\n\n\n\nLorenz system\nThe last example we will use is the Lorenz equations which are famous for their beatiful plots illustrating chaotic dynamics. They originally came from a reduced model for fluid dynamics and take the form:\n\\begin{align}\n\\frac{dx}{dt} &= \\sigma(y - x) \\\\\n\\frac{dy}{dt} &= x(\\rho - z) - y \\\\\n\\frac{dz}{dt} &= xy - \\beta z\n\\end{align}\nwhere x, y, and z are the state variables, and \\sigma, \\rho, and \\beta are the system parameters.\n\n\nCode\nclass Lorenz(nn.Module):\n    \"\"\" \n    Define the Lorenz system as a PyTorch module.\n    \"\"\"\n    def __init__(self, \n                 sigma: float =10.0, # The sigma parameter of the Lorenz system\n                 rho: float=28.0, # The rho parameter of the Lorenz system\n                beta: float=8.0/3, # The beta parameter of the Lorenz system\n                ):\n        super().__init__() \n        self.model_params = torch.nn.Parameter(torch.tensor([sigma, rho, beta]))\n        \n        \n    def forward(self, t, state):\n        x = state[...,0]      #variables are part of vector array u \n        y = state[...,1]\n        z = state[...,2]\n        sol = torch.zeros_like(state)\n        \n        sigma, rho, beta = self.model_params    #coefficients are part of vector array p\n        sol[...,0] = sigma*(y-x)\n        sol[...,1] = x*(rho-z) - y\n        sol[...,2] = x*y - beta*z\n        return sol\n    \n    def __repr__(self):\n        return f\" sigma: {self.model_params[0].item()}, \\\n            rho: {self.model_params[1].item()}, \\\n                beta: {self.model_params[2].item()}\"\n\n\nThis shows how to integrate this system and plot the results. This system (at these parameter values) shows chaotic dynamics so initial conditions that start off close together diverge from one another exponentially.\n\n\nCode\nlorenz_model = Lorenz()\nts = torch.linspace(0,50.0,3000)\nbatch_size = 30\n# Create a batch of initial conditions (batch_dim, state_dim) as small perturbations around one value\ninitial_conditions = torch.tensor([[1.0,0.0,0.0]]) + 0.10*torch.randn((batch_size,3))\nsol = odeint(lorenz_model, initial_conditions, ts, method='dopri5').detach().numpy()\n\n# Check the solution\nplt.plot(ts[:2000], sol[:2000,:,0], lw=0.5);\nplt.title(\"Time series of the Lorenz system\");\nplt.xlabel(\"time\");\nplt.ylabel(\"x\");\n\n\n\n\n\nHere we show the famous butterfly plot (phase plane plot) for the first set of initial conditions in the batch.\n\n\nCode\nplt.plot(sol[:,0,0], sol[:,0,1], color='black', lw=0.5);\nplt.title(\"Phase plot of the Lorenz system\");\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");"
  },
  {
    "objectID": "posts/pytorch-ode/ode.html#data",
    "href": "posts/pytorch-ode/ode.html#data",
    "title": "Differential Equations as a Pytorch Neural Network Layer",
    "section": "Data",
    "text": "Data\nNow that we can define the differential equation models in pytorch we need to create some data to be used in training. This is where things start to get really neat as we see our first glimpse of being able to hijack deep learning machinery for fitting the parameters. Really we could just use tensor of data directly, but this is a nice way to organize the data. It will also be useful if you have some experimental data that you want to use.\nTorch provides the  Dataset  class for loading in data. To use it you just need to create a subclass and define two methods. The __len__ function that returns the number of data points and a __getitem__ function that returns the data point at a given index. If you are wondering these methods are what underly the len(array) and ’array[0]` subscript access in python lists.\nThe rest of boilerplate code needed in defined in the parent class torch.utils.data.Dataset. We will see the power of these method when we go to define a training loop.\n\n\nCode\nclass SimODEData(Dataset):\n    \"\"\" \n        A very simple dataset class for simulating ODEs\n    \"\"\"\n    def __init__(self,\n                 ts: List[torch.Tensor], # List of time points as tensors\n                 values: List[torch.Tensor], # List of dynamical state values (tensor) at each time point \n                 true_model: Union[torch.nn.Module,None] = None,\n                 ) -&gt; None:\n        self.ts = ts \n        self.values = values \n        self.true_model = true_model\n        \n    def __len__(self) -&gt; int:\n        return len(self.ts)\n    \n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        return self.ts[index], self.values[index]\n\n\nNext let’s create a quick generator function to generate some simulated data to test the algorithms on. In a real use case the data would be loaded from a file or database, but for this example we will just generate some data. In fact, I recommend that you always start with generated data to make sure your code is working before you try to load real data.\n\n\nCode\ndef create_sim_dataset(model: nn.Module, # model to simulate from\n                       ts: torch.Tensor, # Time points to simulate for\n                       num_samples: int = 10, # Number of samples to generate\n                       sigma_noise: float = 0.1, # Noise level to add to the data\n                       initial_conditions_default: torch.Tensor = torch.tensor([0.0, 0.0]), # Default initial conditions\n                       sigma_initial_conditions: float = 0.1, # Noise level to add to the initial conditions\n                       ) -&gt; SimODEData:\n    ts_list = [] \n    states_list = [] \n    dim = initial_conditions_default.shape[0]\n    for i in range(num_samples):\n        x0 = sigma_initial_conditions * torch.randn((1,dim)).detach() + initial_conditions_default\n        ys = odeint(model, x0, ts).squeeze(1).detach() \n        ys += sigma_noise*torch.randn_like(ys)\n        ys[0,:] = x0 # Set the first value to the initial condition\n        ts_list.append(ts)\n        states_list.append(ys)\n    return SimODEData(ts_list, states_list, true_model=model)\n\n\nThis just takes in a differential equation model with some initial states and generates some time-series data from it (and adds in some gaussian noise). This data is then passed into our custom dataset container. Let’s define a couple of functions to visualize the model fits.\n\n\nCode\ndef plot_time_series(true_model: torch.nn.Module, # true underlying model for the simulated data\n                     fit_model: torch.nn.Module, # model fit to the data\n                     data: SimODEData, # data set to plot (scatter)\n                     time_range: tuple = (0.0, 30.0), # range of times to simulate the models for\n                     ax: plt.Axes = None, \n                     dyn_var_idx: int = 0,\n                     title: str = \"Model fits\",\n                     *args,\n                     **kwargs) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plot the true model and fit model on the same axes.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n        \n    vdp_model = VDP(mu = 0.10) \n    ts = torch.linspace(time_range[0], time_range[1], 1000)\n    ts_data, y_data = data\n\n    initial_conditions = y_data[0, :].unsqueeze(0)\n    sol_pred = odeint(fit_model, initial_conditions, ts, method='dopri5').detach().numpy()\n    sol_true = odeint(true_model, initial_conditions, ts, method='dopri5').detach().numpy()\n        \n    ax.plot(ts, sol_pred[:,:,dyn_var_idx], color='skyblue', lw=2.0, label='Predicted', **kwargs);\n    ax.scatter(ts_data.detach(), y_data[:,dyn_var_idx].detach(), color='black', s=30, label='Data',  **kwargs);\n    ax.plot(ts, sol_true[:,:,dyn_var_idx], color='black', ls='--', lw=1.0, label='True model', **kwargs);\n    ax.set_title(title);\n    ax.set_xlabel(\"t\");\n    ax.set_ylabel(\"y\");\n    plt.legend();\n    return fig, ax\n\n\n\n\nCode\ndef plot_phase_plane(true_model: torch.nn.Module, # true underlying model for the simulated data\n                     fit_model: torch.nn.Module, # model fit to the data\n                     data: SimODEData, # data set to plot (scatter)\n                     time_range: tuple = (0.0, 30.0), # range of times to simulate the models for\n                     ax: plt.Axes = None, \n                     dyn_var_idx: tuple = (0,1),\n                     title: str = \"Model fits\",\n                     *args,\n                     **kwargs) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plot the true model and fit model on the same axes.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n        \n    ts = torch.linspace(time_range[0], time_range[1], 1000)\n    ts_data, y_data = data\n    \n    initial_conditions = y_data[0, :].unsqueeze(0)\n    sol_pred = odeint(fit_model, initial_conditions, ts, method='dopri5').detach().numpy()\n    sol_true = odeint(true_model, initial_conditions, ts, method='dopri5').detach().numpy()\n    \n    idx1, idx2 = dyn_var_idx\n    \n    ax.plot(sol_pred[:,:,idx1], sol_pred[:,:,idx2], color='skyblue', lw=1.0, label='Fit model');\n    ax.scatter(y_data[:,idx1], y_data[:,idx2].detach(), color='black', s=30, label='Data');\n    ax.plot(sol_true[:,:,idx1], sol_true[:,:,idx2], color='black', ls='--', lw=1.0, label='True model');\n    ax.set_xlabel(r'$x$')\n    ax.set_ylabel(r'$y$')\n    ax.set_title(title)\n    return fig, ax"
  },
  {
    "objectID": "posts/pytorch-ode/ode.html#training-loop",
    "href": "posts/pytorch-ode/ode.html#training-loop",
    "title": "Differential Equations as a Pytorch Neural Network Layer",
    "section": "Training Loop",
    "text": "Training Loop\nNext we will create a wrapper function for a pytorch training loop. Training means we want to update the model parameters to increase the alignment with the data ( or decrease the misalignment).\nOne of the tricks for this from deep learning is to not use all the data before taking a gradient step. Part of this is necessity for using enormous datasets as you can’t fit all of that data inside a GPU’s memory, but this also can help the gradient descent algorithm avoid getting stuck in local minima.\nThe training loop in words: * Divide the dataset into mini-batches, these are subsets of your entire data set. Usually want to choose these randomly. * Iterate through the mini-batches, for each mini-batch: * Generate the predictions using the current model parameters * Calculate the loss (here we will use the mean squared error) * Calculate the gradients, using backpropagation.\n* Update the parameters using a gradient descent step. Here we use the Adam optimizer. * Each full pass through the dataset is called an epoch.\nOkay here is the code:\n\n\nCode\ndef train(model: torch.nn.Module, # Model to train\n          data: SimODEData, # Data to train on\n          lr: float = 1e-2, # learning rate for the Adam optimizer\n          epochs: int = 10, # Number of epochs to train for\n          batch_size: int = 5, # Batch size for training\n          method = 'rk4', # ODE solver to use\n          step_size: float = 0.10, # for fixed diffeq solver set the step size\n          show_every: int = 10, # How often to print the loss function message\n          save_plots_every: Union[int,None] = None, # save a plot of the fit, to disable make this None\n          model_name: str = \"\", #string for the model, used to reference the saved plots \n          *args: tuple, \n          **kwargs: dict\n          ):\n    \n    # Create a data loader to iterate over the data. This takes in our dataset and returns batches of data\n    trainloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n    # Choose an optimizer. Adam is a good default choice as a fancy gradient descent\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    # Create a loss function this computes the error between the predicted and true values\n    criterion = torch.nn.MSELoss() \n    \n    for epoch in range(epochs):\n        running_loss = 0.0 \n        for batchdata in trainloader:\n            optimizer.zero_grad() # reset gradients, famous gotcha in a pytorch training loop\n            ts, states = batchdata # unpack the data \n            initial_state = states[:,0,:] # grab the initial state\n            # Make the prediction and then flip the dimensions to be (batch, state_dim, time)\n            # Pytorch expects the batch dimension to be first\n            pred = odeint(model, initial_state, ts[0], method=method, options={'step_size': step_size}).transpose(0,1) \n            # Compute the loss\n            loss = criterion(pred, states)\n            # compute gradients\n            loss.backward() \n            # update parameters\n            optimizer.step() \n            running_loss += loss.item() # record loss\n        if epoch % show_every == 0:\n            print(f\"Loss at {epoch}: {running_loss}\")\n        # Use this to save plots of the fit every save_plots_every epochs\n        if save_plots_every is not None and epoch % save_plots_every == 0:\n            with torch.no_grad():\n                fig, ax = plot_time_series(data.true_model, model, data[0])\n                ax.set_title(f\"Epoch: {epoch}\")\n                fig.savefig(f\"./tmp_plots/{epoch}_{model_name}_fit_plot\")\n                plt.close()"
  },
  {
    "objectID": "posts/pytorch-ode/ode.html#examples",
    "href": "posts/pytorch-ode/ode.html#examples",
    "title": "Differential Equations as a Pytorch Neural Network Layer",
    "section": "Examples",
    "text": "Examples\n\nFitting the VDP Oscillator\nLet’s use this training loop to recover the parameters from simulated VDP oscillator data.\n\n\nCode\ntrue_mu = 0.30\nmodel_sim = VDP(mu=true_mu)\nts_data = torch.linspace(0.0,10.0,10) \ndata_vdp = create_sim_dataset(model_sim, \n                              ts = ts_data, \n                              num_samples=10, \n                              sigma_noise=0.01)\n\n\nLet’s create a model with the wrong parameter value and visualize the starting point.\n\n\nCode\nvdp_model = VDP(mu = 0.10) \nplot_time_series(model_sim, \n                 vdp_model, \n                 data_vdp[0], \n                 dyn_var_idx=1, \n                 title = \"VDP Model: Before Parameter Fits\");\n\n\n\n\n\nNow, we will use the training loop to fit the parameters of the VDP oscillator to the simulated data.\n\n\nCode\ntrain(vdp_model, data_vdp, epochs=50, model_name=\"vdp\");\nprint(f\"After training: {vdp_model}, where the true value is {true_mu}\")\nprint(f\"Final Parameter Recovery Error: {vdp_model.mu - true_mu}\")\n\n\nLoss at 0: 0.1756787821650505\nLoss at 10: 0.008743234211578965\nLoss at 20: 0.0012590152909979224\nLoss at 30: 0.00026920452364720404\nLoss at 40: 0.0002005345668294467\nAfter training:  mu: 0.3009839951992035, where the true value is 0.3\nFinal Parameter Recovery Error: 0.0009839832782745361\n\n\nNot to bad! Let’s see how the plot looks now…\n\n\nCode\nplot_time_series(model_sim, vdp_model, data_vdp[0], dyn_var_idx=1, title = \"VDP Model: Before Parameter Fits\");\n\n\n\n\n\nThe plot confirms that we almost perfectly recovered the parameter. One more quick plot, where we plot the dynamics of the system in the phase plane (a parametric plot of the state variables).\n\n\nCode\nplot_phase_plane(model_sim, vdp_model, data_vdp[0], title = \"VDP Model: After Fitting\");\n\n\n\n\n\nFigure 1 shows the results of the fit.\n\n\n\nFigure 1: Model fitting for a VDP equation model\n\n\n\n\nLotka Voltera Equations\nNow lets adapt our methods to fit simulated data from the Lotka Voltera equations.\n\n\nCode\nmodel_sim_lv = LotkaVolterra(1.5,1.0,3.0,1.0)\nts_data = torch.arange(0.0, 10.0, 0.1)\ndata_lv = create_sim_dataset(model_sim_lv, \n                              ts = ts_data, \n                              num_samples=10, \n                              sigma_noise=0.1,\n                              initial_conditions_default=torch.tensor([2.5, 2.5]))\n\n\n\n\nCode\nmodel_lv = LotkaVolterra(alpha=1.6, beta=1.1,delta=2.7, gamma=1.2) \n\nplot_time_series(model_sim_lv, model_lv, data = data_lv[0], title = \"Lotka Volterra: Before Fitting\");\n\n\n\n\n\n\n\nCode\ntrain(model_lv, data_lv, epochs=60, lr=1e-2, model_name=\"lotkavolterra\")\nprint(f\"Fitted model: {model_lv}\")\nprint(f\"True model: {model_sim_lv}\")\n\n\nLoss at 0: 1.1350820064544678\nLoss at 10: 0.1333734095096588\nLoss at 20: 0.047438858076930046\nLoss at 30: 0.02459188550710678\nLoss at 40: 0.022399690933525562\nLoss at 50: 0.02224074024707079\nFitted model:  alpha: 1.5970245599746704,             beta: 1.046509027481079,                 delta: 2.8160459995269775,                     gamma: 0.939906895160675\nTrue model:  alpha: 1.5,             beta: 1.0,                 delta: 3.0,                     gamma: 1.0\n\n\n\n\nCode\nplot_time_series(model_sim_lv, model_lv, data = data_lv[0], title = \"Lotka Volterra: After Fitting\");\n\n\n\n\n\nNow let’s visualize the results using a phase plane plot.\n\n\nCode\nplot_phase_plane(model_sim_lv, model_lv, data_lv[0], title= \"Phase Plane for Lotka Volterra: After Fitting\");\n\n\n\n\n\nFigure 2 shows the results of the fit.\n\n\n\nFigure 2: Model fitting visual for the Lotka-Volterra system\n\n\n\n\nLorenz Equations\nFinally, let’s try to fit the Lorenz equations.\n\n\nCode\nmodel_sim_lorenz = Lorenz(sigma=10.0, rho=28.0, beta=8.0/3.0)\nts_data = torch.arange(0, 10.0, 0.05)\ndata_lorenz = create_sim_dataset(model_sim_lorenz, \n                              ts = ts_data, \n                              num_samples=30, \n                              initial_conditions_default=torch.tensor([1.0, 0.0, 0.0]),\n                              sigma_noise=0.01, \n                              sigma_initial_conditions=0.10)\n\n\n\n\nCode\nlorenz_model = Lorenz(sigma=10.2, rho=28.2, beta=9.0/3) \nfig, ax = plot_time_series(model_sim_lorenz, lorenz_model, data_lorenz[0], title=\"Lorenz Model: Before Fitting\");\n\nax.set_xlim((2,15));\n\n\n\n\n\n\n\nCode\ntrain(lorenz_model, \n      data_lorenz, \n      epochs=300, \n      batch_size=5,\n      method = 'rk4',\n      step_size=0.05,\n      show_every=50,\n      lr = 1e-3)\n\n\nLoss at 0: 112.87475204467773\nLoss at 50: 4.314377665519714\nLoss at 100: 2.0217812061309814\nLoss at 150: 1.2296464890241623\nLoss at 200: 0.768804207444191\nLoss at 250: 0.5259109809994698\n\n\nLet’s look at the results from the fitting procedure. Starting with a full plot of the dynamics.\n\n\nCode\nfig, ax = plot_time_series(model_sim_lorenz, lorenz_model, data_lorenz[0], title = \"Lorenz Model: After Fitting\"); \n\n\n\n\n\nLet’s zoom in on the bulk of the data and see how the fit looks.\n\n\nCode\nfig, ax = plot_time_series(model_sim_lorenz, lorenz_model, data_lorenz[0], title = \"Lorenz Model: After Fitting\"); \nax.set_xlim((2,20));\n\n\n\n\n\nYou can see the model is very close to the true model for the data range. Now the phase plane plot.\n\n\nCode\nplot_phase_plane(model_sim_lorenz, lorenz_model, data_lorenz[0], title = \"Lorenz Model: After Fitting\", time_range=(0,20.0));\n\n\n\n\n\nYou can see that our fitted model performs well for t in [0,17] and then starts to diverge."
  },
  {
    "objectID": "posts/pytorch-ode/ode.html#conclusions-and-wrap-up",
    "href": "posts/pytorch-ode/ode.html#conclusions-and-wrap-up",
    "title": "Differential Equations as a Pytorch Neural Network Layer",
    "section": "Conclusions and Wrap-Up",
    "text": "Conclusions and Wrap-Up\nIn this article I have demonstrated how we can use differential equation models within the pytorch ecosytem using the torchdiffeq package. The code from this article is available on  github  and can be opened directly to google colab for experimentation. You can also install the code from this article using pip (pip install paramfittorchdemo).\nThis post is an introduction in the future I will be writing more about the following topics:\n\nHow to blend some mechanistic knowledge of the dynamics with deep learning. These have been called  universal differential equations  as they enable us to combine scientific knowledge with deep learning. This basically blends the two approaches together.\nHow to combine differential equation layers with other deep learning layers.\nModel discovery: Can we recover the actual model equations from data? This uses tools like  SINDy  to extract the model equations from data.\nMLOps tools for managing the training of these models. This includes tools like  MLFlow ,  Weights and Biases , and  Tensorboard .\nAnything else I hear back about from you!\n\nHappy modeling!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Research Interests\n\nMachine Learning\nData Science\nCircadian rhythms and sleep\nBlending physiological mathematical models with deep learning\n\n\n\nEducation\n\n\nUniversity of Michigan\nPhD in Applied and Interdisciplinary Mathematics Advisors: Victoria Booth and Danny Forger\n\n\nAnn Arbor, MI Granted May 2017\n\n\n\nDissertation: Macroscopic Models and Phase Resetting of Coupled Biological Oscillators\nWinner of 2017 Smereka Prize for best dissertation in the department\n\n\n\nUniversity of Texas\nB.S.(Honors) in Mathematics B.S.(Honors) in Biology\n\n\nAustin, TX Granted May 2009\n\n\n\n\nExperience\n\n\nArcascope\n\n\nSeptember 2020 - April 2023\n\n\nChief Technology Officer\n\nCofounder and CTO for a mobile health start-up spun out of my academic research. I officially joined the company when awarded a SBIR grant ($1.6 million) funding the development of an iOS app providing light timing recommendations to treat chronic fatigue in chemotherapy patients.\nIn my first three months I created and implemented the core algorithms used by the company to predict the users internal circadian phase based on wearable data streams. I created a novel method machine learning technique used for the extraction of weak periodic signals from non-stationary time series to extract estimates from heart rate time-series collected by common wearable devices. I also created the models used to predict uncertainty, combine predictions between approaches and handle chronic missing data. This work formed the basis for the companies core intellectual property and the first patent submitted.\nThe next six months focused on scaling and moving these algorithms into production to support the clinical trial. I oversaw the companies first developer hires and stepped into a technical management role as we developed the cloud architecture for the clinical trial app based on a microservices design paradigm. I supervised the development of the iOS app and took an active and supervisory role in the back-end development. The clinical trial app was finished three months ahead of schedule and has run for over 100 participants over the last year without any significant outages or bugs while requiring minimal maintenance.\nThe next three months were focused on customer discovery and algorithm development to support a pivot towards a beachhead market of shift workers. This is a much more challenging technical problem and required the development of a new algorithm for lighting recommendations which accounts for user’s constraints. I then acted as a primary developer and project manager for the development of our second iOS app which was moved from first line of code to MVP in three months- more than doubling the development velocity of the first app with the same team.\nIn the next nine months I was heavily involved in pitching investors (30+ times) the shift worker app and ensuring the technology was respondent to the demand signal generated from defense ,healthcare, airline and lighting verticals. In September 2022, Arcascope closed an oversubscribed seed-round of 2.7 million to bring the shift worker app to market. The app was soft launched in the app store in September 2022 and we are currently cultivating B2B partners in the health care sector.\nDuring this time I also developed core machine learning algorithms for the classification of shift work schedules from users wearable data, sleep classification using wearable data streams and hybrid models for blending deep-learning with physiological constraints. I have also helped build the full-stack machine learning pipeline from ETL through model training, and deployment using the AWS stack.\n\n\n\nUniversity of Michigan \n\n\nSeptember 2020 - June 2021\n\n\nVisiting Assistant Professor\n\nConducted research on circadian rhythms and sleep under the supervision of Dr. Danny Forger\nDeveloped mathematical models of SNS Pain under the supervision of Dr. Victoria Booth\n\n\n\nSchreiner University\n\n\nAugust 2017 - August 2020\n\n\nAssistant Professor of Mathematics\n\nReceived the 2020 Society for Industrial and Applied Mathematics Young Investigator award for research for research on low-dimensional representations of oscillating systems.\nCo-Principal Investigator on National Science Foundation Grant ($500k) to study mathematical models for circadian and sleep jointly with collaborators at the Colorado School of Mines and the University of Michigan.\nConducted research on blending domain knowledge codified by traditional applied mathematics models and artificial neural networks. Developed a simulation platform for human circadian rhythms and statistical models for the prediction of circadian dynamics using wearable data streams.\nTaught mathematics and introductory statistics/data science courses to undergraduate students. Advised the administration on the enrollment and faculty load calculations and data collection pipelines for student retention/churn.\nAuthored open source textbook on data science and introductory statistics for undergraduate students.\n\n\n\nUniversity of Michigan\n\n\nSeptember 2013 - June 2017\n\n\nGraduate student\n\nDoctoral studies in Applied Mathematics, research in coupled oscillator networks, phase response curves and circadian rhythms.\nCompleted coursework in applied mathematics, mathematical biology, numerical analysis and high performance computing.\nReceived the Smereka award for the best doctoral thesis in applied mathematics in 2017\nReceived the Society for Industrial and Applied Mathematics (SIAM) Life Sciences young investigator award in 2020."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin Hannay, PhD",
    "section": "",
    "text": "I’m an experienced data scientist, mathematician, technical leader and just general problem solver. My years in academia provided me with a depth of expertise in applied mathematics and scientific computing and a base of knowledge which enables me to quickly get up to speed in new fields. My years of teaching the fundamentals of data science and statistics helped me develop as a technical communicator and become “brilliant in the basics”, as we say in the Marine Corps.\nSpeaking of which, my years as an active-duty Marine Officer and subsequent service as a reservist have trained me in leadership, organization and planning. More recently, they have also challenged me technically as I moved into the cyberspace field. My experience at Arcascope has challenged me to grow in all these areas. I have learned the fundamental skills of moving code and models into production both hands-on and from a leadership prospective. This has taught me data science pipelining/ ML Ops, AWS cloud architecture, iOS mobile development, DevOps and leading an Agile/SCRUM team of developers."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Differential Equations as a Pytorch Neural Network Layer\n\n\n\n\n\n\n\ndeep learning\n\n\ndata science\n\n\ndifferential equations\n\n\n\n\n“A tutorial on how to use differential equations as a pytorch neural network layer. We will use the torchdiffeq library to solve the differential equations.”\n\n\n\n\n\n\nApr 8, 2023\n\n\nKevin Hannay\n\n\n\n\n\n\n  \n\n\n\n\nEntrainment Signal Regularity Index: Measuring Circadian Disruption\n\n\n\n\n\n\n\ncircadian\n\n\ndata science\n\n\n\n\n“A summary of the Entrainment Signal Regularity Index (ESRI) and how it can be used to measure circadian disruption.”\n\n\n\n\n\n\nFeb 2, 2023\n\n\nKevin Hannay\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "circadian\nTools for the analysis and simulation of circadian rhythms in python.\n\n        \n        Website\n     \n        \n        Github\n     \n        \n        Package\n    \nShift\nMobile (iOS) app for the prediction of circadian phase of shift workers and building optimal light schedules to align them to a target schedule.\n\n        \n        Website\n    \nparamfittorchdemo\nExamples of fitting dynamical system parameters using tools from Machine Learning.\n\n        \n        Website\n     \n        \n        Github\n     \n        \n        Package\n    \nHannayIntroStats\nR package written to accompany my open source textbook on introducing multidiscplinary undergraduates to data science\n\n        \n        Github\n    \nFittingParamsDiffEqFlux\nTutorial code for how to fit differential equations models using the deep learning library Flux in Julia\n\n        \n        Github"
  }
]